/*
Test experience. Use a self-hosted Ollama server with a Llama model

For a tutor plugin for self-hosting a cpu only ollama.
See: https://gist.github.com/felipemontoya/509495d3fbaa696fa2b684880a8388da
*/
{
  "orchestrator_class": "DirectLLMResponse",
  "processor_config": {
    "OpenEdXProcessor": {
    },
    "LLMProcessor": {
      "function": "greet_from_llm",
      "config": "ollama"
    }
  },
  "actuator_config": {
    "UIComponents": {
      "request": {
        "component": "AIRequestComponent",
        "config": {
          "buttonText": "Hello AI",
          "customMessage": "Ollama will say hello"
        }
      },
      "response": {
        "component": "AIResponseComponent",
        "config": {
          "customMessage": "Ollama Response"
        }
      }
    }
  },
  "schema_version": "1.0",
}
